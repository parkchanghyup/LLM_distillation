# Data paths
data:
  raw_data_path: "data/raw/*.json"
  processed_data_path: "data/processed/processed_data.json"
  summaries_path: "data/summaries/"
  train_data_path: "data/processed/train_data.json"
  test_data_path: "data/processed/test_data.json"
  inference_results_path: "data/results/"
  max_input_length: 2000

# Model configuration
model:
  LLM:
    name: "facebook/opt-125m"  # or any other model from Hugging Face
    max_seq_length: 2048
  sLLM:
    name: "google/gemma-2-2b-it"
    max_seq_length: 2048

# Summarization settings
summarization:
  max_length: 300
  min_length: 50
  temperature: 0.4
  do_sample: true
  top_k: 50
  top_p: 0.95

# Training configuration
training:
  output_dir: "models/trained_model"
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  num_train_epochs: 3
  eval_steps: 500
  logging_steps: 100
  learning_rate: 2e-5
  warmup_steps: 500
  max_steps: -1  # -1 means train for num_train_epochs

# Inference configuration
inference:
  batch_size: 16
  max_length: 150
  num_return_sequences: 1
