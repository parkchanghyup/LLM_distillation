# Data paths
data:
  raw_data_path: "data/raw/*.json"
  summaries_path: "data/summaries/"
  test_data_path: "data/test_data.json"
  max_input_length: 2000

# Model configuration
model:
  LLM:
    name: "google/gemma-2-27b-it"  # or any other model from Hugging Face
    max_seq_length: 2048
  sLLM:
    name: "google/gemma-2-2b-it"
    max_seq_length: 2048

# Summarization settings
summarization:
  max_length: 2000
  min_length: 50
  temperature: 0.4
  do_sample: true
  top_k: 50
  top_p: 0.95

# Training configuration
training:
  output_dir: "models/trained_model"
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  num_train_epochs: 2
  eval_steps: 500
  logging_steps: 100
  learning_rate: 2e-4
  warmup_steps: 500
  test_size: 0.2

